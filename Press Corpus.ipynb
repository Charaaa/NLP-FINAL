{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:37:05.238609Z",
     "start_time": "2020-07-07T12:37:03.428826Z"
    }
   },
   "outputs": [],
   "source": [
    "# basic operations\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from pprint import pprint as pp\n",
    "# data analysis/management/manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# nlp pipeline\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import stanza \n",
    "# building corpus/dictionary\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:36:50.932300Z",
     "start_time": "2020-07-07T12:36:50.929053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy version: 2.0.12\n",
      "Gensim version: 3.8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "spaCy version: {}\n",
    "Gensim version: {}\n",
    "\"\"\".format(spacy.__version__, gensim.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:36:58.035851Z",
     "start_time": "2020-07-07T13:36:58.033242Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "FOLDER = 'brexit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:17:23.999971Z",
     "start_time": "2020-07-07T12:17:23.959191Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE1 = 'pr__sus_attr.csv'\n",
    "FILE2 = 'pr__sus_docs.csv'\n",
    "FILE3 = 'pr__sources.csv'\n",
    "pr_attr = pd.read_csv(os.path.join(PATH, FOLDER, FILE1))\n",
    "pr_docs = pd.read_csv(os.path.join(PATH, FOLDER, FILE2))\n",
    "pr_sources = pd.read_csv(os.path.join(PATH, FOLDER, FILE3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:17:25.716085Z",
     "start_time": "2020-07-07T12:17:25.706877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'source', 'variable', 'value'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_attr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:35:31.577134Z",
     "start_time": "2020-07-07T12:35:31.573934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'text', 'article', 'start', 'id', 'sort'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_docs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:17:28.534185Z",
     "start_time": "2020-07-07T12:17:28.506564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>id</th>\n",
       "      <th>sort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>647</td>\n",
       "      <td>2016-04-23 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>HIGHLIGHT: Ed Crooks is fascinated by a biogra...</td>\n",
       "      <td>HIGHLIGHT: Ed Crooks is fascinated by a biogra...</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>646</td>\n",
       "      <td>2016-04-23 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>HIGHLIGHT: \\'If China could be persuaded to co...</td>\n",
       "      <td>HIGHLIGHT: \\'If China could be persuaded to co...</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>645</td>\n",
       "      <td>2016-04-23 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>After months of wrangling, eurozone finance mi...</td>\n",
       "      <td>After months of wrangling, eurozone finance mi...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>644</td>\n",
       "      <td>2016-04-23 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>How can the world best combat global warming? ...</td>\n",
       "      <td>How can the world best combat global warming? ...</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>648</td>\n",
       "      <td>2016-04-26 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Zaoui &amp; Co, the tiny European advisory firm se...</td>\n",
       "      <td>Zaoui &amp; Co, the tiny European advisory firm se...</td>\n",
       "      <td>599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>765</td>\n",
       "      <td>2016-08-17 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Ministers have given the go-ahead for the worl...</td>\n",
       "      <td>Ministers have given the go-ahead for the worl...</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>619</td>\n",
       "      <td>2016-08-18 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Sustainable palm oil buyers are still struggli...</td>\n",
       "      <td>Sustainable palm oil buyers are still struggli...</td>\n",
       "      <td>436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>766</td>\n",
       "      <td>2016-08-18 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Timing is everything when it comes to investin...</td>\n",
       "      <td>Timing is everything when it comes to investin...</td>\n",
       "      <td>545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>767</td>\n",
       "      <td>2016-08-19 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>When the furnaces were turned off at South Aus...</td>\n",
       "      <td>When the furnaces were turned off at South Aus...</td>\n",
       "      <td>581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>768</td>\n",
       "      <td>2016-08-19 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Sir, Joseph Stiglitz argues that \" A split eur...</td>\n",
       "      <td>Sir, Joseph Stiglitz argues that \" A split eur...</td>\n",
       "      <td>414</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     article                 date  source  \\\n",
       "445      647  2016-04-23 00:00:00       3   \n",
       "444      646  2016-04-23 00:00:00       3   \n",
       "443      645  2016-04-23 00:00:00       3   \n",
       "442      644  2016-04-23 00:00:00       3   \n",
       "446      648  2016-04-26 00:00:00       3   \n",
       "..       ...                  ...     ...   \n",
       "562      765  2016-08-17 00:00:00       3   \n",
       "441      619  2016-08-18 00:00:00       3   \n",
       "563      766  2016-08-18 00:00:00       3   \n",
       "564      767  2016-08-19 00:00:00       3   \n",
       "565      768  2016-08-19 00:00:00       3   \n",
       "\n",
       "                                                  text  \\\n",
       "445  HIGHLIGHT: Ed Crooks is fascinated by a biogra...   \n",
       "444  HIGHLIGHT: \\'If China could be persuaded to co...   \n",
       "443  After months of wrangling, eurozone finance mi...   \n",
       "442  How can the world best combat global warming? ...   \n",
       "446  Zaoui & Co, the tiny European advisory firm se...   \n",
       "..                                                 ...   \n",
       "562  Ministers have given the go-ahead for the worl...   \n",
       "441  Sustainable palm oil buyers are still struggli...   \n",
       "563  Timing is everything when it comes to investin...   \n",
       "564  When the furnaces were turned off at South Aus...   \n",
       "565  Sir, Joseph Stiglitz argues that \" A split eur...   \n",
       "\n",
       "                                                 start   id  sort  \n",
       "445  HIGHLIGHT: Ed Crooks is fascinated by a biogra...  180     1  \n",
       "444  HIGHLIGHT: \\'If China could be persuaded to co...  225     1  \n",
       "443  After months of wrangling, eurozone finance mi...   20     1  \n",
       "442  How can the world best combat global warming? ...  233     1  \n",
       "446  Zaoui & Co, the tiny European advisory firm se...  599     1  \n",
       "..                                                 ...  ...   ...  \n",
       "562  Ministers have given the go-ahead for the worl...  322     1  \n",
       "441  Sustainable palm oil buyers are still struggli...  436     1  \n",
       "563  Timing is everything when it comes to investin...  545     1  \n",
       "564  When the furnaces were turned off at South Aus...  581     1  \n",
       "565  Sir, Joseph Stiglitz argues that \" A split eur...  414     1  \n",
       "\n",
       "[566 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_date = pr_attr[pr_attr.variable == 'date'].drop(['variable','source'], axis=1)\n",
    "\n",
    "# the expected timespan is April 23, 2016 - August 23, 2016\n",
    "pr_timespan = pr_date[(pr_date.value < '2016-08-24 00:00:00') & (pr_date.value >= '2016-04-23 00:00:00')]\n",
    "\n",
    "# get list of chosen article id\n",
    "articles = pr_timespan.article.to_list()\n",
    "\n",
    "# join pr_docs and pr_timespan\n",
    "pr = pd.merge(pr_timespan, pr_docs, on=['article', 'article']).rename(columns={'value':'date'})\n",
    "pr.sort_values('date', inplace=True)\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:17:30.755573Z",
     "start_time": "2020-07-07T12:17:30.685850Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare list to pass through spacy\n",
    "docs = [article.strip().lower() for article in pr.text]\n",
    "\n",
    "# hyphen to underscores\n",
    "docs = [re.sub(r'\\b-\\b', '_', text) for text in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple 'web_lg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:34:22.925718Z",
     "start_time": "2020-07-07T13:34:16.598042Z"
    }
   },
   "outputs": [],
   "source": [
    "# load spaCy model 'web_lg'\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:34:23.508137Z",
     "start_time": "2020-07-07T13:34:23.504230Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand on spaCy's stopwords\n",
    "# my stopwrods\n",
    "my_stopwords = ['\\x1c',\n",
    "                'ft', 'wsj', 'time', 'sec',\n",
    "                'say', 'says', 'said',\n",
    "                'mr.', 'mister', 'mr', 'miss', 'ms',\n",
    "                'inc']\n",
    "# expand on spacy's stopwords\n",
    "for stopword in my_stopwords:\n",
    "    nlp.vocab[stopword].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:35:07.186632Z",
     "start_time": "2020-07-07T13:34:24.248451Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "docs_tokens, tmp_tokens = [], []\n",
    "\n",
    "for doc in docs:\n",
    "    tmp_tokens = [token.lemma_ for token in nlp(doc)\n",
    "                  if not token.is_stop\n",
    "                  and not token.is_space\n",
    "                  and not token.is_punct\n",
    "                  and not token.like_num\n",
    "                  and not token.like_url\n",
    "                  and not token.like_email\n",
    "                  and not token.is_currency\n",
    "                  and not token.is_oov]\n",
    "    docs_tokens.append(tmp_tokens)\n",
    "    tmp_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:36:02.326467Z",
     "start_time": "2020-07-07T13:35:59.860614Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(docs, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:36:06.428733Z",
     "start_time": "2020-07-07T13:36:03.818528Z"
    }
   },
   "outputs": [],
   "source": [
    "# get rid of common terms\n",
    "common_terms = [u'of', u'with', u'without', u'and', u'or', u'the', u'a',\n",
    "                u'not', 'be', u'to', u'this', u'who', u'in']\n",
    "\n",
    "# fing phrases as bigrams\n",
    "bigram = Phrases(docs_tokens,\n",
    "                 min_count=50,\n",
    "                 threshold=5,\n",
    "                 max_vocab_size=50000,\n",
    "                 common_terms=common_terms)\n",
    "# fing phrases as trigrams\n",
    "trigram = Phrases(bigram[docs_tokens],\n",
    "                  min_count=50,\n",
    "                  threshold=5,\n",
    "                  max_vocab_size=50000,\n",
    "                  common_terms=common_terms)\n",
    "# manipulate docs\n",
    "docs_phrased = [bigram[line] for line in docs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:38:45.096938Z",
     "start_time": "2020-07-07T13:38:45.093208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/omoi/Documents/SMM694-NLP'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:39:23.818853Z",
     "start_time": "2020-07-07T13:39:23.206445Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_dictionary = Dictionary(docs_phrased)\n",
    "pr_dictionary.save('/Users/omoi/Documents/SMM694-NLP/corpus/pr_dictionary.dict')\n",
    "\n",
    "pr_corpus = [pr_dictionary.doc2bow(doc) for doc in docs_phrased]\n",
    "\n",
    "corpora.MmCorpus.serialize('/Users/omoi/Documents/SMM694-NLP/corpus/pr_corpus.mm', pr_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T12:32:39.915309Z",
     "start_time": "2020-07-07T12:32:39.910885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================\n",
      "published text: highlight: \\'if china could be persuaded to construct energy_efficient new buildings, we might have an effective way of reducing global warming\\'  how can the world best combat global warming? when asked that question, many people will talk about the need to close coal_fired power stations, embrace electric cars, recycle our rubbish and eschew air travel. some might also point out that it is not enough for reforms to occur only in the west - emerging economies are also crucial. but if the former us treasury secretary hank paulson is correct, there is another essential step that almost nobody is talking about: putting money into chinese housing. yes, you read that right. these days, the phrase \"chinese real estate\" is usually mentioned by economists who are worried about bubbles. but paulson is fascinated by another concern. recent environmental studies suggest that about \"40 per cent of carbon emissions currently come from buildings\", or so he told me over lunch earlier this month. meanwhile, \"roughly half of new buildings going up in the world [now] are going up in china\", he continued, because of the breathless pace of development.  thus, if only the world could persuade china to construct those new buildings in an energy_efficient way, we might have the means to reduce global warming. or, to put it another way, if you want to make the world more green, targeting the way that china constructs its buildings might be more effective than endlessly fretting about buildings in countries where the structures are already built. is this just a crazy idea? you might be tempted to think so. after all, paulson shot to fame as the former goldman sachs ceo who became the republican treasury secretary and then had to deal with the subprime mortgage market shock of 2008. at the time, he seemed like a tough_talking, all_american financial mandarin, the man who created a self_styled policy \"bazooka\" - a $700bn stimulus package. (when a tv movie, too big to fail, was made about the crisis paulson was played by william hurt at his most rugged.) paulson\\'s background is not an obvious platform from which to start a campaign to change the way that china builds its houses; least of all when other republicans such as donald trump are winning votes with china_bashing. but paulson is embarking on his mission via his own foundation, the paulson institute. his theory is that china, if it is ever going to turn green, needs to find ways to make environmentally suitable buildings more cost effective. so he is trying to raise finance from public and private investors, inside china and elsewhere, to subsidise the use of green products, as part of a range of other environmental initiatives. \"if you look at china, the bad news is that in their energy mix, roughly 70 per cent is coal, 20 per cent is oil and only 10 per cent is renewables,\" he argues. \"but the good news is that there is huge opportunity - and building is just one example.\" who knows if this might work? but it is intriguing on several levels. first, it highlights an oft_ignored point: that if we want to tackle climate change, issues such as domestic recycling or electric cars are only a tiny part of the mix. second, it shows that old_fashioned philanthropy is no longer the only game in town; nor, for that matter, are multilateral funds (or world bank largesse). instead, the fashion these days is finding ways to get private investors to back socially worthy projects, via innovations such as \"green\" bonds. this is hippy idealism meets goldman sachs spreadsheets. but there is a third aspect to all this: paulson\\'s tale is a welcome example of middle_aged reinvention. over lunch, he admitted that when he left office in 2008, he suffered a bout of the blues: without the adrenaline rush of power, it was tough to cope with the endless criticism of his policies. once, former leaders in his position might head for the golf course; more recently, they have tried to establish general do_good foundations (such as the clinton global initiative) or become consultants. but paulson\\'s strategy of picking a specific project - environmental challenges in china - seems unusually focused; today he exudes the air of a man who is profoundly at ease with himself. and who knows? he may end up doing some real good for the world too - either by getting more green buildings in china or even giving a new slant to the us_china relationship. (unsurprisingly, paulson is horrified by the tenor and tone of the trump campaign and is supporting the candidacy of john kasich.) either way, the sight of a us treasury secretary who once grappled with american subprime housing woes now worrying about chinese cement is unexpected - and oddly cheering. financial history can move in some unexpected ways. gillian.tett@ft.com\n",
      "\n",
      "=============================================================================\n",
      "tokenized text: ['highlight', 'china', 'could', 'be', 'persuade', 'to', 'construct', 'new', 'building', '-PRON-', 'may', 'have', 'an', 'effective', 'way', 'of', 'reduce', 'global', 'how', 'can', 'the', 'world', 'good', 'combat', 'global', 'warming', 'when', 'ask', 'that', 'question', 'many', 'people', 'will', 'talk', 'about', 'the', 'need', 'to', 'close', 'power', 'station', 'embrace', 'electric', 'car', 'recycle', '-PRON-', 'rubbish', 'and', 'eschew', 'air', 'travel', 'some', 'may', 'also', 'point', 'out', 'that', '-PRON-', 'be', 'not', 'enough', 'for', 'reform', 'to', 'occur', 'only', 'in', 'the', 'west', 'emerge', 'economy', 'be', 'also', 'crucial', 'but', 'if', 'the', 'former', '-PRON-', 'treasury', 'secretary', 'hank', 'paulson', 'be', 'correct', 'there', 'be', 'another', 'essential', 'step', 'that', 'almost', 'nobody', 'be', 'talk', 'about', 'put', 'money', 'into', 'chinese', 'housing', 'yes', '-PRON-', 'read', 'that', 'right', 'these', 'day', 'the', 'phrase', 'chinese', 'real', 'estate', 'be', 'usually', 'mention', 'by', 'economist', 'who', 'be', 'worried', 'about', 'bubble', 'but', 'paulson', 'be', 'fascinate', 'by', 'another', 'concern', 'recent', 'environmental', 'study', 'suggest', 'that', 'about', 'per', 'cent', 'of', 'carbon', 'emission', 'currently', 'come', 'from', 'building', 'or', 'so', '-PRON-', 'tell', '-PRON-', 'over', 'lunch', 'earlier', 'this', 'month', 'meanwhile', 'roughly', 'half', 'of', 'new', 'building', 'go', 'up', 'in', 'the', 'world', 'now', 'be', 'go', 'up', 'in', 'china', '-PRON-', 'continue', 'because', 'of', 'the', 'breathless', 'pace', 'of', 'development', 'thus', 'if', 'only', 'the', 'world', 'could', 'persuade', 'china', 'to', 'construct', 'those', 'new', 'building', 'in', 'an', 'way', '-PRON-', 'may', 'have', 'the', 'mean', 'to', 'reduce', 'global', 'warming', 'or', 'to', 'put', '-PRON-', 'another', 'way', 'if', '-PRON-', 'want', 'to', 'make', 'the', 'world', 'more', 'green', 'target', 'the', 'way', 'that', 'china', 'construct', '-PRON-', 'building', 'may', 'be', 'more', 'effective', 'than', 'endlessly', 'fret', 'about', 'building', 'in', 'country', 'where', 'the', 'structure', 'be', 'already', 'build', 'be', 'this', 'just', 'a', 'crazy', 'idea', '-PRON-', 'may', 'be', 'tempt', 'to', 'think', 'so', 'after', 'all', 'paulson', 'shoot', 'to', 'fame', 'as', 'the', 'former', 'goldman', 'sachs', 'ceo', 'who', 'become', 'the', 'republican', 'treasury', 'secretary', 'and', 'then', 'have', 'to', 'deal', 'with', 'the', 'subprime', 'mortgage', 'market', 'shock', 'of', 'at', 'the', '-PRON-', 'seem', 'like', 'a', 'financial', 'mandarin', 'the', 'man', 'who', 'create', 'a', 'policy', 'bazooka', 'a', '$', '700bn', 'stimulus', 'package', 'when', 'a', 'tv', 'movie', 'too', 'big', 'to', 'fail', 'be', 'make', 'about', 'the', 'crisis', 'paulson', 'be', 'play', 'by', 'william', 'hurt', 'at', '-PRON-', 'most', 'rugged', \"'s\", 'background', 'be', 'not', 'an', 'obvious', 'platform', 'from', 'which', 'to', 'start', 'a', 'campaign', 'to', 'change', 'the', 'way', 'that', 'china', 'build', '-PRON-', 'house', 'least', 'of', 'all', 'when', 'other', 'republican', 'such', 'as', 'donald', 'trump', 'be', 'win', 'vote', 'with', 'but', 'paulson', 'be', 'embark', 'on', '-PRON-', 'mission', 'via', '-PRON-', 'own', 'foundation', 'the', 'paulson', 'institute', '-PRON-', 'theory', 'be', 'that', 'china', 'if', '-PRON-', 'be', 'ever', 'go', 'to', 'turn', 'green', 'need', 'to', 'find', 'way', 'to', 'make', 'environmentally', 'suitable', 'building', 'more', 'cost', 'effective', 'so', '-PRON-', 'be', 'try', 'to', 'raise', 'finance', 'from', 'public', 'and', 'private', 'investor', 'inside', 'china', 'and', 'elsewhere', 'to', 'subsidise', 'the', 'use', 'of', 'green', 'product', 'as', 'part', 'of', 'a', 'range', 'of', 'other', 'environmental', 'initiative', 'if', '-PRON-', 'look', 'at', 'china', 'the', 'bad', 'news', 'be', 'that', 'in', '-PRON-', 'energy', 'mix', 'roughly', 'per', 'cent', 'be', 'coal', 'per', 'cent', 'be', 'oil', 'and', 'only', 'per', 'cent', 'be', 'renewable', '-PRON-', 'argue', 'but', 'the', 'good', 'news', 'be', 'that', 'there', 'be', 'huge', 'opportunity', 'and', 'building', 'be', 'just', 'example', 'who', 'know', 'if', 'this', 'may', 'work', 'but', '-PRON-', 'be', 'intriguing', 'on', 'several', 'level', 'first', '-PRON-', 'highlight', 'an', 'point', 'that', 'if', '-PRON-', 'want', 'to', 'tackle', 'climate', 'change', 'issue', 'such', 'as', 'domestic', 'recycling', 'or', 'electric', 'car', 'be', 'only', 'a', 'tiny', 'part', 'of', 'the', 'mix', 'second', '-PRON-', 'show', 'that', 'philanthropy', 'be', 'no', 'longer', 'the', 'only', 'game', 'in', 'town', 'nor', 'for', 'that', 'matter', 'be', 'multilateral', 'fund', 'or', 'world', 'bank', 'largesse', 'instead', 'the', 'fashion', 'these', 'day', 'be', 'find', 'way', 'to', 'get', 'private', 'investor', 'to', 'back', 'socially', 'worthy', 'project', 'via', 'innovation', 'such', 'as', 'green', 'bond', 'this', 'be', 'hippy', 'idealism', 'meet', 'goldman', 'sachs', 'spreadsheet'"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 18569 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check outcome of nlp pipeline\n",
    "print('''\n",
    "=============================================================================\n",
    "published text: {}\n",
    "\n",
    "=============================================================================\n",
    "tokenized text: {}\n",
    "\n",
    "=============================================================================\n",
    "tri-grammed tokenized text: {}\n",
    "\n",
    "'''.format(docs[1],\n",
    "           docs_tokens[1],\n",
    "           docs_phrased[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:26:45.493962Z",
     "start_time": "2020-07-07T14:26:45.344756Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a list to pass through the Stanza pipeline\n",
    "def cleaning(_string):\n",
    "    '''\n",
    "    : argument     : string 's'\n",
    "    : return clean : clean version of 's' (lower case, no non-alpha characters)\n",
    "    '''\n",
    "    # purge non alpha characters\n",
    "    alpha = re.sub(\"[^A-Za-z']+\", ' ', str(_string))\n",
    "    return alpha.lower()\n",
    "\n",
    "\n",
    "# get a list\n",
    "docs = [cleaning(item) for item in pr.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:24:35.689129Z",
     "start_time": "2020-07-07T14:24:33.345198Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-07 15:24:33 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-07-07 15:24:33 INFO: Use device: cpu\n",
      "2020-07-07 15:24:33 INFO: Loading: tokenize\n",
      "2020-07-07 15:24:33 INFO: Loading: pos\n",
      "2020-07-07 15:24:33 INFO: Loading: lemma\n",
      "2020-07-07 15:24:34 INFO: Loading: depparse\n",
      "2020-07-07 15:24:34 INFO: Loading: ner\n",
      "2020-07-07 15:24:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# download English model (once for all)\n",
    "#stanza.download('en') \n",
    "\n",
    "# initialize English neural pipeline\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:42:40.692519Z",
     "start_time": "2020-07-07T14:42:40.689096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor doc in docs:\\n    for i, sentence in enumerate(nlp(doc).sentences):\\n        print(f'___________ Sentence {i+1} tokens ____________')\\n        print(*[f'id: {token.id}\\ttext: {token.text}' \\n                for token in sentence.tokens], sep='\\n')\\n\\n\\ntokens = []\\n\\nfor i, doc in enumerate(docs):\\n    for j, sentence in enumerate(nlp(doc).sentences):\\n        tmp_tokens = [t.text for t in sentence.tokens]\\n        tokens.append([i, [j, tmp_tokens]])\\n\\n\\n\\nTOO MUCH TIME TO RUN WITHOUT GPU\\n\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for doc in docs:\n",
    "    for i, sentence in enumerate(nlp(doc).sentences):\n",
    "        print(f'___________ Sentence {i+1} tokens ____________')\n",
    "        print(*[f'id: {token.id}\\ttext: {token.text}' \n",
    "                for token in sentence.tokens], sep='\\n')\n",
    "\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(nlp(doc).sentences):\n",
    "        tmp_tokens = [t.text for t in sentence.tokens]\n",
    "        tokens.append([i, [j, tmp_tokens]])\n",
    "\n",
    "\n",
    "\n",
    "TOO MUCH TIME TO RUN WITHOUT GPU\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:24:15.874968Z",
     "start_time": "2020-07-07T13:24:15.870089Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy_stanza import StanzaLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:39:54.828384Z",
     "start_time": "2020-07-07T14:39:54.148554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-07 15:39:54 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-07-07 15:39:54 INFO: Use device: cpu\n",
      "2020-07-07 15:39:54 INFO: Loading: tokenize\n",
      "2020-07-07 15:39:54 INFO: Loading: pos\n",
      "2020-07-07 15:39:54 INFO: Loading: lemma\n",
      "2020-07-07 15:39:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "snlp = stanza.Pipeline(lang=\"en\", processors='tokenize,pos,lemma')\n",
    "nlp = StanzaLanguage(snlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:51:20.415701Z",
     "start_time": "2020-07-07T14:44:53.892867Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a2b28180de7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtoken_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msents\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtoken_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrehearse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_default_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy_stanza/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mthese\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmapped\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvecs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_token_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vector\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy_stanza/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0msnlp_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnlp_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0msnlp_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnlp_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokens_with_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnlp_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    174\u001b[0m         assert any([isinstance(doc, str), isinstance(doc, list),\n\u001b[1;32m    175\u001b[0m                     isinstance(doc, Document)]), 'input should be either str, list or Document'\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_seqlen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizeProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SEQ_LENGTH_DEFAULT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                                    \u001b[0morig_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                                    no_ssplit=self.config.get('no_ssplit', False))\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/models/tokenize/utils.py\u001b[0m in \u001b[0;36moutput_predictions\u001b[0;34m(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchparas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/models/tokenize/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/stanza/models/tokenize/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, feats)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv_res'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "for line in docs:\n",
    "    doc = nlp.pipe([line])\n",
    "    token_details = []\n",
    "\n",
    "    for sents in doc:\n",
    "        for tok in sents:\n",
    "            token_details.append([tok.text])\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
